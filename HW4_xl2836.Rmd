---
title: "Homework 4"
author: "Xinyi Lin"
date: "11/9/2018"
output: html_document
---
# Problem 1

# Question 2

As 

```{r}
library(tidyverse)
library(patchwork)
```

# Problem 2

First, we need to import data

```{r}
HeartDisease_df = read_csv("./data/HeartDisease.csv") 

head(HeartDisease_df)
```

## Question 1

This dataset includes `r nrow(HeartDisease_df)` observations and `r ncol(HeartDisease_df)` variables. Among variables, main outcome is `totalcost` and main predictor is `ERvisits`. 

Then, we show descriptive statistics for all variables of interest.

```{r}
mean_and_sd = function(x) {
  
  if (!is.numeric(x)) {
    stop("Argument x should be numeric")
  } else if (length(x) == 1) {
    stop("Cannot be computed for length 1 vectors")
  }
  
  mean_x = mean(x)
  sd_x = sd(x)

  list(mean = mean_x, 
       sd = sd_x)
}
```

`totalcost`

```{r}
mean_and_sd(HeartDisease_df$totalcost)
```

`ERvisits`

```{r}
mean_and_sd(HeartDisease_df$ERvisits)
```

`age`

```{r}
mean_and_sd(HeartDisease_df$age)
```

`gender`

```{r}
summary(as.factor(HeartDisease_df$gender))
```

`complications`

```{r}
summary(as.factor(HeartDisease_df$complications))
```

## Question 2

```{r}
total_plot =
  HeartDisease_df %>% 
  ggplot(aes(x = totalcost)) +
  geom_density() +
  labs(title = "pdf of total cost")
```

```{r}
log_plot = 
  HeartDisease_df %>% 
  ggplot(aes(x = log(totalcost))) +
  geom_density() +
  labs(title = "pdf of log(total cost)")
```

```{r}
sqrt_plot = 
  HeartDisease_df %>% 
  ggplot(aes(x = sqrt(totalcost))) +
  geom_density() +
  labs(title = "pdf of square root of total cost")
```

```{r}
square_plot = 
  HeartDisease_df %>% 
  ggplot(aes(x = totalcost^2)) +
  geom_density() +
  labs(title = "pdf of total cost square")
```

```{r}
(total_plot + square_plot)/(log_plot + sqrt_plot)
```

Above are distribution of total cost, log(totalcost), suqre root of totalcost and totalcost square. We can find that apply log to total cost is the best transformations.

## Question 3

```{r}
HeartDisease_df =
  HeartDisease_df %>% 
  mutate(comp_bin = ifelse(complications == 0, 0, 1)) %>% 
  mutate(totalcost = ifelse(totalcost == 0, 0.001, totalcost))

head(HeartDisease_df)
```

## Question 4

```{r}
HeartDisease_df %>% 
  mutate(log_totalcost = log(totalcost)) %>% 
  ggplot(aes(x = log_totalcost, y = ERvisits)) +
  geom_point() +
  geom_smooth(method = 'lm',formula = y~x)
```

```{r}
reg_Heart = 
  HeartDisease_df %>% 
  mutate(log_totalcost = log(totalcost)) %>% 
  #filter(is.finite(log_totalcost)) %>% 
  lm(formula = log_totalcost ~ ERvisits, data = .) 

reg_Heart %>% 
  broom::tidy()

summary(reg_Heart)
```

According to the results, we can find that adjusted R-squared is 0.1014 which is very closed to 0 and means this simple linear model is not a proper model. However, p-value is lower than 2.2e-16, which means the slope is significant and there are positive relationship between log of total cost and number of emergency room visits.

Interpretation: The slop of model is 0.227 which means if the number of emergency room vistis increases by 1 unit, the log of total cost will increase 0.452 units.

## Question 5

Test if `comp_bin` is an effect modifier

```{r}
reg_modifier_Heart = 
  HeartDisease_df %>% 
  mutate(log_totalcost = log(totalcost)) %>% 
  #filter(is.finite(log_totalcost)) %>% 
  lm(formula = log_totalcost ~ ERvisits + comp_bin + ERvisits*comp_bin, data = .) 

reg_modifier_Heart %>% 
  broom::tidy()

summary(reg_modifier_Heart)
```

Since the corresponding p-value of 'ERvisits*comp_bin' is 0.357 which is bigger than 0.05, we can conclude that there is no interaction between `ERvisits` and `comp_bin` and `comp_bin` is not a modifier.

Test if `comp_bin` is a confunder.

```{r}
reg_confounder_Heart = 
  HeartDisease_df %>% 
  mutate(log_totalcost = log(totalcost)) %>% 
  #filter(is.finite(log_totalcost)) %>% 
  lm(formula = log_totalcost ~ ERvisits + comp_bin, data = .) 

reg_confounder_Heart %>% 
  broom::tidy()

summary(reg_confounder_Heart)
```

When adding `comp_bin` in model the association between `log_totalcost` and `ERvisits` becomes smaller but still significant and the regression coefficient decreased by 10.2%, so `comp_bin` is a confounder.

Since `comp_bin` is a confounder but not a modifier, we use 'Partial' F-test to test whether we should include `comp_bin` as a factor.

Model 1: $Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \varepsilon_i$

Model 2: $Y_i = \beta_0 + \beta_1X_{i1} + \varepsilon_i$

Among which, $X_1$ represents `ER_visits`,  $X_2$ represents `comp_bin`.

Null hypothesis $H_O: \beta_2 = 0$, alternative hypothesis $H_1: \beta_2 \neq 0$

```{r}
anova(reg_confounder_Heart, reg_Heart) %>% 
  broom::tidy()
```

According to results, p-value is smaller than 0.01 so we reject $H_0$ and conclude that Model 1 is 'superior'.As a resuit, we should include `comp_bin`.

## Question 6

```{r}
reg_added_Heart = 
  HeartDisease_df %>% 
  mutate(log_totalcost = log(totalcost)) %>% 
  #filter(is.finite(log_totalcost)) %>% 
  lm(formula = log_totalcost ~ ERvisits + comp_bin + age + gender + duration, data = .) 

reg_added_Heart %>% 
  broom::tidy()

summary(reg_added_Heart)
```

According to results, we can find all p-value of covariates are smaller than 0.01, so all covariates have significant influence in total cost.

We use 'Partial' F-test to compare SLR and MLR models.

Model 1: $Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_3X_{i3} + \beta_4X_{i4} + \beta_5X_{i5} + \varepsilon_i$

Model 2: $Y_i = \beta_0 + \beta_1X_{i1} + \varepsilon_i$

Among which, $X_1$ represents `ERvisits`,  $X_2$ represents `comp_bin`,  $X_3$ represents `age`, $X_4$ represents `gender`, $X_5$ represents `duration`.

Null hypothesis $H_O: \beta_2 = \beta_3 = \beta_4 = \beta_5 = 0$, alternative hypothesis $H_1:$ at least one of $\beta$ is not zero.

```{r}
anova(reg_Heart, reg_added_Heart) %>% broom::tidy()
```

According to the ANOVA results, p-value is smaller than 0.01 so we reject $H_0$ and conclude that Model 1 is 'superior'.As a resuit, we should use MLR model.

# Problem 3

First, we import data

```{r}
PatSatisfaction_df = readxl::read_xlsx("./data/PatSatisfaction.xlsx") %>% 
  janitor::clean_names() %>% 
  reshape::rename(c(safisfaction = "satisfaction"))

head(PatSatisfaction_df)
```

## Question 1

```{r}
PatSatisfaction_df %>% 
  cor()
```

According to the correlation matrix, we can find that all `age`, `severity`, `anxirty` have negative relationship with satisfaction and the relationship between `age` and `satisfaction` is stronger than `severity` and `anxiety` 

## Question 2

Assuming the model is 
$$ Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_3X_{i3} + \varepsilon_i $$
Among which, $X_1$ represents `age`,  $X_2$ represents `severity`,  $X_3$ represents `anxiety`.

Null hypothesis $H_0 : \beta_0 = \beta_1 = \beta_2 = \beta_3 =0$, alternative hypothesis $H_1 :$ at least one $\beta$ is not zero.

```{r}
reg_all = 
  PatSatisfaction_df %>%
  lm(satisfaction ~ age + severity + anxiety, data = .)

summary(reg_all)
anova(reg_all)
```

## Question 5

Model 1: $Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_3X_{i3} + \varepsilon_i$

Model 2: $Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \varepsilon_i$

Among which, $X_1$ represents `age`,  $X_2$ represents `severity`,  $X_3$ represents `anxiety`.

We use 'Partial' F-test for nested models. Null hypothesis $H_O: \beta_3 = 0$, alternative hypothesis $H_1: \beta_3 \neq 0$

```{r}
reg_without_anxiety = 
  PatSatisfaction_df %>%
  lm(satisfaction ~ age + severity, data = .)

anova(reg_all, reg_without_anxiety)
```

According to the ANOVA results, p-value is 0.0647 which is larger than 0.05, so we fail to reject $H_0$ and conclude that Model 1 is not 'superior` and we should use Model 2.