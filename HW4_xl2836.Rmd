---
title: "Homework 4"
author: "Xinyi Lin"
date: "11/9/2018"
output:
  word_document: default
  pdf_document: default
  html_document: default
---
# Problem 1

## Question 1

$$ b_1 = \frac{n\sum x_iY_i-\sum x_i\sum Y_i}{n\sum x^2_i-(\sum x_i)^2} = \frac{\sum x_iY_i-n\hat{Y}\bar{x}}{\sum x_i^2 - n\bar{x}^2} $$

$$ b_0 = \hat{Y} - b_1 \bar{x} $$
Since

$$ \sum x_iY_i - n\bar{Y}\bar{x} = \sum x_iY_i-\bar{x}\sum Y_i = \sum(x_i-\bar{x})Y_i $$

So, the expectation of $b_1$'s numerator is

$$
\begin{split}
E\{\sum (x_i-\bar{x})Y_i\} & =\sum (x_i-\bar{x})E(Y_i)\\
& =\sum(x_i - \bar{x})(\beta_0+\beta_1x_i)\\
& =\beta_0\sum x_i-n\bar{x}\beta_0+\beta_1\sum{x_i}^2-n\bar{x}^2\beta_1\\
& =\beta_1(\sum{x_i^2}-n\bar{x}^2)
\end{split} 
$$

So
\begin{split}
E(b_1) &=\frac{E\{\sum{(x_i-\bar{x})Y_i}\}}{\sum{x_i^2}-n\bar{x}^2}\\
&=\frac{\beta_1(\sum{x_i^2}-n\bar{x}^2)}{\sum{x_i^2}-n\bar{x}^2}\\
&=\beta_1\\

E(b_0) &= E(\hat{Y}-b_1\bar{x})\\
&=\frac{1}{n}\sum{E(Y_i)}-E(b_1)\bar{x}\\
&=\frac{1}{n}\sum{[\beta_0+\beta_1x_1]}-\beta_1\bar{x}\\
&=\frac{1}{n}[n\beta_0+n\beta_1\bar{x}]-\beta_1\bar{x}\\
&=\beta_0
\end{split}

So $b_1$ and $b_0$ are unbiased estimators of $\beta_1$ and $\beta_0$.

## Question 2

As $\hat{\beta_0}=\bar{Y}-\hat{\beta_1}$, $b_1 = \frac{n\sum x_iY_i-\sum x_i\sum Y_i}{n\sum x^2_i-(\sum x_i)^2} = \frac{\sum x_iY_i-n\hat{Y}\bar{x}}{\sum x_i^2 - n\bar{x}^2}$ and estimated regression model $\hat{Y_i}=\hat{\beta_0}+\hat{\beta_1}x_i$,

when $x_i=\bar{x}$,

$$\hat{Y_i}=\bar{Y}-\hat{\beta_1}\bar{x}+\hat{\beta_1}\bar{x}=\bar{Y}$$
so regression model always goes through the point $(\bar{x},\bar{y})$.

```{r, message = FALSE}
library(tidyverse)
library(patchwork)
```

# Problem 2

First, we need to import data

```{r}
HeartDisease_df = read_csv("./data/HeartDisease.csv") 

head(HeartDisease_df)
```

## Question 1

This dataset includes `r nrow(HeartDisease_df)` observations and `r ncol(HeartDisease_df)` variables. Among variables, main outcome is `totalcost` and main predictor is `ERvisits`. 

Then, we show descriptive statistics for all variables of interest.

```{r}
mean_and_sd = function(x) {
  
  if (!is.numeric(x)) {
    stop("Argument x should be numeric")
  } else if (length(x) == 1) {
    stop("Cannot be computed for length 1 vectors")
  }
  
  mean_x = mean(x)
  sd_x = sd(x)

  list(mean = mean_x, 
       sd = sd_x)
}
```

`totalcost`

```{r}
mean_and_sd(HeartDisease_df$totalcost)
```

`ERvisits`

```{r}
mean_and_sd(HeartDisease_df$ERvisits)
```

`age`

```{r}
mean_and_sd(HeartDisease_df$age)
```

`gender`

```{r}
summary(as.factor(HeartDisease_df$gender))
```

`complications`

```{r}
summary(as.factor(HeartDisease_df$complications))
```

## Question 2

```{r}
total_plot =
  HeartDisease_df %>% 
  ggplot(aes(x = totalcost)) +
  geom_density() +
  labs(title = "pdf of total cost")
```

```{r}
log_plot = 
  HeartDisease_df %>% 
  ggplot(aes(x = log(totalcost))) +
  geom_density() +
  labs(title = "pdf of log(total cost)")
```

```{r}
sqrt_plot = 
  HeartDisease_df %>% 
  ggplot(aes(x = sqrt(totalcost))) +
  geom_density() +
  labs(title = "pdf of square root of total cost")
```

```{r}
square_plot = 
  HeartDisease_df %>% 
  ggplot(aes(x = totalcost^2)) +
  geom_density() +
  labs(title = "pdf of total cost square")
```

```{r}
(total_plot + square_plot)/(log_plot + sqrt_plot)
```

Above are distribution of total cost, log(totalcost), suqre root of totalcost and totalcost square. We can find that apply log to total cost is the best transformations.

## Question 3

```{r}
HeartDisease_df =
  HeartDisease_df %>% 
  mutate(comp_bin = ifelse(complications == 0, 0, 1)) %>% 
  mutate(totalcost = ifelse(totalcost == 0, 0.001, totalcost))

head(HeartDisease_df)
```

## Question 4

```{r}
HeartDisease_df %>% 
  mutate(log_totalcost = log(totalcost)) %>% 
  ggplot(aes(x = log_totalcost, y = ERvisits)) +
  geom_point() +
  geom_smooth(method = 'lm',formula = y~x)
```

```{r}
reg_Heart = 
  HeartDisease_df %>% 
  mutate(log_totalcost = log(totalcost)) %>% 
  #filter(is.finite(log_totalcost)) %>% 
  lm(formula = log_totalcost ~ ERvisits, data = .) 

reg_Heart %>% 
  broom::tidy()

summary(reg_Heart)
```

According to the results, we can find that adjusted R-squared is 0.1014 which is very closed to 0 and means this simple linear model is not a proper model. However, p-value is lower than 2.2e-16, which means the slope is significant and there are positive relationship between log of total cost and number of emergency room visits.

Interpretation: The slop of model is 0.227 which means if the number of emergency room vistis increases by 1 unit, the log of total cost will increase 0.452 units.

## Question 5

Test if `comp_bin` is an effect modifier

```{r}
reg_modifier_Heart = 
  HeartDisease_df %>% 
  mutate(log_totalcost = log(totalcost)) %>% 
  #filter(is.finite(log_totalcost)) %>% 
  lm(formula = log_totalcost ~ ERvisits + comp_bin + ERvisits*comp_bin, data = .) 

reg_modifier_Heart %>% 
  broom::tidy()

summary(reg_modifier_Heart)
```

Since the corresponding p-value of 'ERvisits*comp_bin' is 0.357 which is bigger than 0.05, we can conclude that there is no interaction between `ERvisits` and `comp_bin` and `comp_bin` is not a modifier.

Test if `comp_bin` is a confunder.

```{r}
reg_confounder_Heart = 
  HeartDisease_df %>% 
  mutate(log_totalcost = log(totalcost)) %>% 
  #filter(is.finite(log_totalcost)) %>% 
  lm(formula = log_totalcost ~ ERvisits + comp_bin, data = .) 

reg_confounder_Heart %>% 
  broom::tidy()

summary(reg_confounder_Heart)
```

When adding `comp_bin` in model the association between `log_totalcost` and `ERvisits` becomes smaller but still significant and the regression coefficient decreased by 10.2%, so `comp_bin` is a confounder.

Since `comp_bin` is a confounder but not a modifier, we use 'Partial' F-test to test whether we should include `comp_bin` as a factor.

Model 1: $Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \varepsilon_i$

Model 2: $Y_i = \beta_0 + \beta_1X_{i1} + \varepsilon_i$

Among which, $X_1$ represents `ER_visits`,  $X_2$ represents `comp_bin`.

Null hypothesis $H_O: \beta_2 = 0$, alternative hypothesis $H_1: \beta_2 \neq 0$

```{r}
anova(reg_confounder_Heart, reg_Heart) %>% 
  broom::tidy()
```

According to results, p-value is smaller than 0.01 so we reject $H_0$ and conclude that Model 1 is 'superior'.As a resuit, we should include `comp_bin`.

## Question 6

```{r}
reg_added_Heart = 
  HeartDisease_df %>% 
  mutate(log_totalcost = log(totalcost)) %>% 
  #filter(is.finite(log_totalcost)) %>% 
  lm(formula = log_totalcost ~ ERvisits + comp_bin + age + gender + duration, data = .) 

reg_added_Heart %>% 
  broom::tidy()

summary(reg_added_Heart)
```

According to results, we can find all p-value of covariates are smaller than 0.01, so all covariates have significant influence in total cost.

We use 'Partial' F-test to compare SLR and MLR models.

Model 1: $Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_3X_{i3} + \beta_4X_{i4} + \beta_5X_{i5} + \varepsilon_i$

Model 2: $Y_i = \beta_0 + \beta_1X_{i1} + \varepsilon_i$

Among which, $X_1$ represents `ERvisits`,  $X_2$ represents `comp_bin`,  $X_3$ represents `age`, $X_4$ represents `gender`, $X_5$ represents `duration`.

Null hypothesis $H_O: \beta_2 = \beta_3 = \beta_4 = \beta_5 = 0$, alternative hypothesis $H_1:$ at least one of $\beta$ is not zero.

```{r}
anova(reg_Heart, reg_added_Heart) %>% broom::tidy()
```

According to the ANOVA results, p-value is smaller than 0.01 so we reject $H_0$ and conclude that Model 1 is 'superior'.As a resuit, we should use MLR model.

# Problem 3

First, we import data

```{r}
PatSatisfaction_df = readxl::read_xlsx("./data/PatSatisfaction.xlsx") %>% 
  janitor::clean_names() %>% 
  reshape::rename(c(safisfaction = "satisfaction"))

head(PatSatisfaction_df)
```

## Question 1

```{r}
PatSatisfaction_df %>% 
  cor()
```

According to the correlation matrix, we can find that all `age`, `severity`, `anxirty` have negative relationship with satisfaction and the relationship between `age` and `satisfaction` is stronger than `severity` and `anxiety` .

## Question 2

Assuming the model is 
$$ Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_3X_{i3} + \varepsilon_i $$
Among which, $X_1$ represents `age`,  $X_2$ represents `severity`,  $X_3$ represents `anxiety`.

Null hypothesis $H_0 : \beta_0 = \beta_1 = \beta_2 = \beta_3 =0$, alternative hypothesis $H_1 :$ at least one $\beta$ is not zero.

Decision rule:

If $F^* = \frac{MSR}{MSE} > F(1-\alpha;p,n-p-1)$, reject $H_0$,

if $F^* = \frac{MSR}{MSE} \leq F(1-\alpha;p,n-p-1)$, fail to reject $H_0$.

with a significance level of 0.05, $\alpha = 0.05$

```{r}
reg_all = 
  PatSatisfaction_df %>%
  lm(satisfaction ~ age + severity + anxiety, data = .)

summary(reg_all)
```

According to results, we can find $F^*$ = 30.05 > `r qf(0.95,3,42)`, so we reject $H_0$ and conclude that there is a regression relation. 

## Question 3

```{r}
reg_all %>% broom::tidy()
confint(reg_all)
```

By using function `confint`, we get 95% CIs of all estimators. The 95% CIs of `severity` is (-1.4348, 0.5508) which means at $\alpha = 0.05$ significant level, we can conclude that the mean value of satisfaction changes somewhere between decreasing 1.4348 and increasing 0.5508 for each additional unit of the severity of the illness given all other values of predictors stay constant. The estimated coefficient of `severity` is -0.442 which means if the value of `severity` increased by 1 units, the mean value of satisfaction will decrease 0.442 given all other values of predictors stay constant.

## Question 4

```{r}
list(age = 35, severity = 42, anxiety = 2.1) %>% 
  predict(object = reg_all, newdata = ., interval = "predict")
``` 

By using `predict` function, we can get the prediction interval for the new patient's satisfaction is (50.0624, 93.3042).

Interprest: We are 95% confident that the the new patient's satisfaction fall within (50.0624, 93.3042) given `age` equals 35, `severity` equals 42 and `anxiety` equals 2.1

## Question 5

Model 1: $Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_3X_{i3} + \varepsilon_i$

Model 2: $Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \varepsilon_i$

Among which, $X_1$ represents `age`,  $X_2$ represents `severity`,  $X_3$ represents `anxiety`.

We use 'Partial' F-test for nested models. Null hypothesis $H_O: \beta_3 = 0$, alternative hypothesis $H_1: \beta_3 \neq 0$

Decision rule:

$$ F^*=\frac{(SSR_L-SSR_S)/(df_L-df_S)}{\frac{SSE_L}{df_L}} \sim F_{df_L-df_S,dfL} $$ 

where $df_S = n-p_S-1, df_L = n-p_L-1$.

If $F^* > F(1-\alpha;df_L-df_S,df_L)$, reject $H_0$;

If $F^* \leq F(1-\alpha;df_L-df_S,df_L)$, fail to reject $H_0$.

With $\alpha = 0.05$, when $p-value \geq 0.05$, fail to reject $H_0$, when $p-value < 0.05$, reject $H_0$.

```{r}
reg_without_anxiety = 
  PatSatisfaction_df %>%
  lm(satisfaction ~ age + severity, data = .)

anova(reg_all, reg_without_anxiety) %>% 
  broom::tidy()
```

According to the ANOVA results, p-value is 0.0647 which is larger than 0.05, so we fail to reject $H_0$ and conclude that Model 1 is not 'superior` and we should use Model 2.